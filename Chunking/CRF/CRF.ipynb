{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.11"
    },
    "colab": {
      "name": "CRF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDUnrDke3Um1"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import tensorflow\n",
        "import numpy\n",
        "from collections import Counter\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM\n",
        "from keras_contrib.layers import CRF\n",
        "from keras_contrib.losses import crf_loss\n",
        "from keras_contrib.metrics import crf_viterbi_accuracy\n",
        "from keras_contrib.datasets import conll2000\n",
        "from keras.utils.data_utils import get_file\n",
        "from zipfile import ZipFile\n",
        "from collections import Counter\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.stem.porter import *\n",
        "from gensim.test.utils import common_texts, get_tmpfile\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import conll2000\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import tensorflow\n",
        "import numpy\n",
        "import keras\n",
        "from keras import Input\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras_contrib.datasets import conll2000\n",
        "from keras.utils.data_utils import get_file\n",
        "import copy\n",
        "\n",
        " \n",
        "train_file = \"train.txt\"\n",
        "test_file = \"test.txt\"\n",
        "output_train_file = \"train_features.txt\"\n",
        "\n",
        "def filex(name):\n",
        "  train_sent_words=[]\n",
        "\n",
        "  with open(name,'r') as f:\n",
        "      lines = f.readlines()\n",
        "      sent = []\n",
        "      for line in lines:\n",
        "          line = line.rstrip()\n",
        "          if line.rstrip() == \"\":\n",
        "              train_sent_words.append(sent)\n",
        "              sent = []\n",
        "          else:\n",
        "              line = line.split(\" \")\n",
        "              word = []\n",
        "              seps = line[2].split(\"-\")\n",
        "              if len(seps) == 2:\n",
        "                  word.append(line[0]);word.append(line[1]);word.append(seps[0]);word.append(seps[1])\n",
        "              else:\n",
        "                  word.append(line[0]);word.append(line[1]);word.append(seps[0]);word.append(\"\")\n",
        "              sent.append(word)\n",
        "  return train_sent_words\n",
        "             \n",
        "            \n",
        "# Word2vec\n",
        "train_sent_words = filex(train_file)\n",
        "test_sent_words = filex(test_file)\n",
        "sents = [list(zip(*sent))[0] for sent in train_sent_words]\n",
        "sents2 = [list(zip(*sent))[0] for sent in test_sent_words]\n",
        "\n",
        "model = Word2Vec(sents+sents2, min_count=1)\n",
        "words = list(model.wv.vocab)\n",
        "model.save('model.bin')\n",
        "new_model = Word2Vec.load('model.bin')\n",
        "\n",
        "\n",
        "# POS\n",
        "\n",
        "pos_tags = [list(zip(*sent))[1] for sent in train_sent_words]\n",
        "pos_tags2 = [list(zip(*sent))[1] for sent in test_sent_words]\n",
        "\n",
        "flat_list = set([item for sublist in pos_tags for item in sublist])\n",
        "unique_tag_list = list(flat_list)\n",
        "number_of_tags = len(unique_tag_list)\n",
        "\n",
        "def create_one_hot_POS(pos):\n",
        "    x = [0]*number_of_tags\n",
        "    x[unique_tag_list.index(pos)] = 1\n",
        "    return x\n",
        "\n",
        "# Chunk labels \n",
        "\n",
        "chunk_tags = ['B','I','O']\n",
        "chunk_labels = [list(zip(*sent))[2] for sent in train_sent_words]\n",
        "chunk_labels2 = [list(zip(*sent))[2] for sent in test_sent_words]\n",
        "def create_one_hot_CL(cl):\n",
        "    x = [0]*3\n",
        "    x[chunk_tags.index(cl)] = 1\n",
        "    return chunk_tags.index(cl)\n",
        "\n",
        "# Morphological features\n",
        "\n",
        "lis_noun_suff = ['acy','al','ance','ence','dom','er','or','ism','ist','ity','ty','ment','ness','ship','tion','sion']\n",
        "lis_verb_suff = ['ate','en','ify','fy','ize','ise','ed','ing','in','ted']\n",
        "lis_adv_suff = ['ly','ward','wise']\n",
        "lis_adj_suff = ['able','ible','al','esque','ful','ic','ical','ious','ous','ish','ive','less','y']\n",
        "\n",
        "stemmer = PorterStemmer() \n",
        "\n",
        "def give_morphological_features(word):\n",
        "    ret = [0]*10\n",
        "    temp = 0\n",
        "    temp = ord(word[0])\n",
        "    temp2 = -5\n",
        "    temp3 = -5\n",
        "    if(len(word)>2):\n",
        "        temp2 = ord(word[-2])\n",
        "        temp3 = ord(word[-3])\n",
        "    flag = 0\n",
        "    flag2 = 0\n",
        "    verb_flag = 0\n",
        "    len_flag = 0\n",
        "    poss_flag = 0\n",
        "    #Suffix\n",
        "    bi_suf = word[-2:]\n",
        "    tri_suf = word[-3:]\n",
        "    four_suf = word[-4:]\n",
        "    stem = stemmer.stem(word)\n",
        "    if((bi_suf in lis_adj_suff or tri_suf in lis_adj_suff or four_suf in lis_adj_suff) and stem!=word):\n",
        "        ret[5] = 1\n",
        "    elif((bi_suf in lis_noun_suff or tri_suf in lis_noun_suff or four_suf in lis_noun_suff) and stem!=word):\n",
        "        ret[6] = 1\n",
        "    elif((bi_suf in lis_adv_suff or tri_suf in lis_adv_suff or four_suf in lis_adv_suff) and stem!=word):\n",
        "        ret[7] = 1 \n",
        "        flag2 = 1\n",
        "    elif((bi_suf in lis_verb_suff or tri_suf in lis_verb_suff or four_suf in lis_verb_suff) and stem!=word):\n",
        "        ret[8] = 1\n",
        "        verb_flag = 1\n",
        "    #Check Hyphen\n",
        "    for jj in range(len(word)-1):\n",
        "        if(ord(word[jj])==45 and ord(word[jj+1])!=45):\n",
        "            ret[4] = 1 \n",
        "            flag = 1\n",
        "            break\n",
        "    #Check Number\n",
        "    if(temp<=57 and temp >47):\n",
        "        ret[0] = 1\n",
        "    #Check First Upper Cap \n",
        "    if(temp<=90 and temp>=65):\n",
        "        ret[1] = 1 \n",
        "    #Check LOwer Cap  \n",
        "    if(len(word)<=3):\n",
        "        ret[2] = 1 \n",
        "        len_flag = 1\n",
        "    if(temp2==39 or temp3==39): \n",
        "        ret[9] = 1\n",
        "        poss_flag = 1\n",
        "    if((temp==46 or temp==96 or temp==95 or temp ==94 or temp==58 or temp ==59 or temp ==33 or temp==34  or temp ==44)):\n",
        "        ret[3] = 1  \n",
        "    return ret\n",
        "\n",
        "morph_features = list(map(lambda x: list(map(lambda y:give_morphological_features(y), x)),sents))\n",
        "morph_features2 = list(map(lambda x: list(map(lambda y:give_morphological_features(y), x)),sents2))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShJyblf73VZD"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "total_feat = []\n",
        "for i,sent in enumerate(sents):\n",
        "  feat1 = list(map(lambda x,y: list(new_model[x])+ create_one_hot_POS(y),sent,pos_tags[i]))\n",
        "  total_feat.append(list(map(lambda x:x[0]+x[1],zip(feat1,morph_features[i]))))\n",
        "total_feat2 = []\n",
        "for i,sent in enumerate(sents2):\n",
        "  feat1 = list(map(lambda x,y: list(new_model[x])+ create_one_hot_POS(y),sent,pos_tags2[i]))\n",
        "  total_feat2.append(list(map(lambda x:x[0]+x[1],zip(feat1,morph_features2[i]))))  \n",
        "maxlen = max(len(s) for s in total_feat)  \n",
        "total_feat = pad_sequences(total_feat, maxlen = 78,value = [0.0]*154,padding = 'post',dtype = 'float32')\n",
        "total_feat2 = pad_sequences(total_feat2, maxlen = 78,value = [0.0]*154,padding = 'post',dtype = 'float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnRyogpd3Yy7"
      },
      "source": [
        "y_train = []\n",
        "for i in range(len(chunk_labels)):\n",
        "  lab = list(map(lambda x:chunk_tags.index(x),chunk_labels[i]))\n",
        "  y_train.append(np.asarray(lab))\n",
        "y_test = []\n",
        "for i in range(len(chunk_labels2)):\n",
        "  lab = list(map(lambda x:chunk_tags.index(x),chunk_labels2[i]))\n",
        "  y_test.append(np.asarray(lab))\n",
        "y_train = pad_sequences(y_train, maxlen = 78,value = 3,padding = 'post',dtype = 'float32')\n",
        "y_test = pad_sequences(y_test, maxlen = 78,value = 3,padding = 'post',dtype = 'float32')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTVlQCswwfL6"
      },
      "source": [
        "total_feat = np.asarray(total_feat)\n",
        "final = np.zeros((8936,78,154*5))\n",
        "for i in range(len(total_feat)):\n",
        "  for j in range(78):\n",
        "    final[i][j][:154] = total_feat[i][j]  \n",
        "    if(j<77):      \n",
        "      final[i][j][154*2:154*3] = total_feat[i][j+1]\n",
        "    if(j>0):      \n",
        "      final[i][j][154:154*2] = total_feat[i][j-1]\n",
        "      final[i][j][154*5:154*5+1] = y_train[i][j-1]\n",
        "    if(j<76):      \n",
        "      final[i][j][154*4:154*5] = total_feat[i][j+2]\n",
        "    if(j>1):      \n",
        "      final[i][j][154*3:154*4] = total_feat[i][j-2]  \n",
        "\n",
        "total_feat2 = np.asarray(total_feat2)\n",
        "final2 = np.zeros((len(total_feat2),78,154*5))\n",
        "for i in range(len(total_feat2)):\n",
        "  for j in range(78):\n",
        "    final2[i][j][:154] = total_feat2[i][j]  \n",
        "    if(j<77):      \n",
        "      final2[i][j][154*2:154*3] = total_feat2[i][j+1]\n",
        "    if(j>0):      \n",
        "      final2[i][j][154:154*2] = total_feat2[i][j-1]\n",
        "    if(j<76):      \n",
        "      final2[i][j][154*4:154*5] = total_feat2[i][j+2]\n",
        "    if(j>1):      \n",
        "      final2[i][j][154*3:154*4] = total_feat2[i][j-2]              "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD_g_6Lp3cEB"
      },
      "source": [
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0ANdGAMiuWr"
      },
      "source": [
        "#CRF Train\n",
        "crf = CRF(4, sparse_target=True)\n",
        "inputs = Input(shape=(78,770, ))\n",
        "outputs = crf(inputs)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "model.compile('adam', loss=crf_loss, metrics=[crf_viterbi_accuracy])\n",
        "total_feat = np.asarray(total_feat)\n",
        "y_train = np.asarray(y_train)\n",
        "y_train = y_train.squeeze()\n",
        "y_train = np.expand_dims(y_train, axis=2)\n",
        "x = final[:7700]\n",
        "valx = final[7700:]\n",
        "y = y_train[:7700,:,:]\n",
        "valy = y_train[7700:]\n",
        "model.fit(x, y, epochs=15,validation_data=[valx,valy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn0xUNfQju_2"
      },
      "source": [
        "total_feat2 = np.asarray(total_feat2)\n",
        "y_test = np.asarray(y_test)\n",
        "y_test = np.expand_dims(y_test, axis=2)\n",
        "test_y_pred = model.predict(final2)\n",
        "test_y_true = y_test\n",
        "chunk_tags = [\"B\",\"I\",\"O\"]\n",
        "ss=np.argmax(test_y_pred,axis=2)\n",
        "coun = 0\n",
        "coun2=0\n",
        "valy = valy.squeeze()\n",
        "for i in range(len(y_test)):\n",
        "  for j in range(78):\n",
        "    if(y_test[i][j]<2):\n",
        "      coun2 = coun2+1\n",
        "      if(y_test[i][j]==ss[i][j]):\n",
        "        coun = coun+1"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CkYYs9z23In"
      },
      "source": [
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import sklearn\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "fig, ax = plt.subplots(figsize=(12,12))   \n",
        "ss = ss.flatten()\n",
        "test_y_true  = test_y_true.flatten()\n",
        "mat = confusion_matrix(test_y_true, ss, labels=[0,1,2,3])\n",
        "labels=['B','I','O','Pad']\n",
        "ax.matshow(mat, cmap=plt.cm.Blues)\n",
        "lis = [-1,0,1,2,3]\n",
        "ax.set_xticks(lis)\n",
        "ax.set_yticks(lis)\n",
        "ax.set_xticklabels([''] + labels)\n",
        "ax.set_yticklabels([''] + labels)\n",
        "for i in range(4):\n",
        "  for j in range(4):\n",
        "    c = mat[j,i]\n",
        "    ax.text(i, j, str(c), va='bottom', ha='center')\n",
        "    ax.text(i,j,str(round((c/mat.sum()*100),2))+\"%\",va='top', ha='center')\n",
        "plt.xlabel('Predicted')\n",
        "ax.xaxis.set_label_position('top')\n",
        "plt.ylabel('True')\n",
        "per_Pos = np.zeros((2))    \n",
        "for i in range(2):\n",
        "  per_Pos[i] = mat[i][i]/np.sum(mat[i])\n",
        "print(\"per_pos\",per_Pos)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJI7mO9U7UX_"
      },
      "source": [
        "# import sys\n",
        "# import tensorflow\n",
        "# import keras\n",
        "# print(sys.version)\n",
        "# print(numpy.__version__)\n",
        "# print(tensorflow.__version__)\n",
        "# print(keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}